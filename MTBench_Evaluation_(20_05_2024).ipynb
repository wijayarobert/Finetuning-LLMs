{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_NXNvoekKo3",
        "outputId": "41446354-48cd-4c25-a936-4eca1094bd1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'FastChat'...\n",
            "remote: Enumerating objects: 7045, done.\u001b[K\n",
            "remote: Total 7045 (delta 0), reused 0 (delta 0), pack-reused 7045\u001b[K\n",
            "Receiving objects: 100% (7045/7045), 33.96 MiB | 14.36 MiB/s, done.\n",
            "Resolving deltas: 100% (5314/5314), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/lm-sys/FastChat.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/FastChat/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bg4J44dktNMo",
        "outputId": "43f6c9b6-a886-4328-ee06-af4516575120"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/FastChat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -e \".[model_worker,llm_judge]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRa3DPHFtkye",
        "outputId": "e5cda830-cea6-4ab8-dab7-afe3142f0c4b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/FastChat\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.36) (3.9.5)\n",
            "Collecting fastapi (from fschat==0.2.36)\n",
            "  Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from fschat==0.2.36)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting markdown2[all] (from fschat==0.2.36)\n",
            "  Downloading markdown2-2.4.13-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nh3 (from fschat==0.2.36)\n",
            "  Downloading nh3-0.2.17-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (777 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m777.1/777.1 kB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.36) (1.25.2)\n",
            "Requirement already satisfied: prompt-toolkit>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.36) (3.0.43)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.36) (2.7.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.36) (5.9.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.36) (2.31.0)\n",
            "Requirement already satisfied: rich>=10.0.0 in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.36) (13.7.1)\n",
            "Collecting shortuuid (from fschat==0.2.36)\n",
            "  Downloading shortuuid-1.0.13-py3-none-any.whl (10 kB)\n",
            "Collecting tiktoken (from fschat==0.2.36)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn (from fschat==0.2.36)\n",
            "  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate>=0.21 (from fschat==0.2.36)\n",
            "  Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting peft (from fschat==0.2.36)\n",
            "  Downloading peft-0.11.1-py3-none-any.whl (251 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.36) (0.1.99)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.36) (2.2.1+cu121)\n",
            "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.36) (4.40.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.36) (3.20.3)\n",
            "Collecting openai<1 (from fschat==0.2.36)\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting anthropic>=0.3 (from fschat==0.2.36)\n",
            "  Downloading anthropic-0.26.0-py3-none-any.whl (877 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m877.7/877.7 kB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ray (from fschat==0.2.36)\n",
            "  Downloading ray-2.22.0-cp310-cp310-manylinux2014_x86_64.whl (65.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21->fschat==0.2.36) (24.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21->fschat==0.2.36) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21->fschat==0.2.36) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21->fschat==0.2.36) (0.4.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from anthropic>=0.3->fschat==0.2.36) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from anthropic>=0.3->fschat==0.2.36) (1.7.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from anthropic>=0.3->fschat==0.2.36) (1.3.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from anthropic>=0.3->fschat==0.2.36) (0.19.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from anthropic>=0.3->fschat==0.2.36) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->fschat==0.2.36) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx->fschat==0.2.36)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->fschat==0.2.36) (3.7)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->fschat==0.2.36)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai<1->fschat==0.2.36) (4.66.4)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit>=3.0.0->fschat==0.2.36) (0.2.13)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->fschat==0.2.36) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic->fschat==0.2.36) (2.18.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fschat==0.2.36) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fschat==0.2.36) (2.0.7)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.0.0->fschat==0.2.36) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.0.0->fschat==0.2.36) (2.16.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->fschat==0.2.36) (3.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->fschat==0.2.36) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->fschat==0.2.36) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->fschat==0.2.36) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->fschat==0.2.36) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->fschat==0.2.36)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->fschat==0.2.36)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->fschat==0.2.36)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->fschat==0.2.36)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->fschat==0.2.36)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->fschat==0.2.36)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->fschat==0.2.36)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->fschat==0.2.36)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->fschat==0.2.36)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch->fschat==0.2.36)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->fschat==0.2.36)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->fschat==0.2.36) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->fschat==0.2.36)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->fschat==0.2.36) (2023.12.25)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->fschat==0.2.36) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->fschat==0.2.36) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->fschat==0.2.36) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->fschat==0.2.36) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->fschat==0.2.36) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->fschat==0.2.36) (4.0.3)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi->fschat==0.2.36)\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi-cli>=0.0.2 (from fastapi->fschat==0.2.36)\n",
            "  Downloading fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\n",
            "Collecting python-multipart>=0.0.7 (from fastapi->fschat==0.2.36)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi->fschat==0.2.36)\n",
            "  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson>=3.2.1 (from fastapi->fschat==0.2.36)\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting email_validator>=2.0.0 (from fastapi->fschat==0.2.36)\n",
            "  Downloading email_validator-2.1.1-py3-none-any.whl (30 kB)\n",
            "Collecting wavedrom (from markdown2[all]->fschat==0.2.36)\n",
            "  Downloading wavedrom-2.0.3.post3.tar.gz (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.7/137.7 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray->fschat==0.2.36) (8.1.7)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray->fschat==0.2.36) (4.19.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray->fschat==0.2.36) (1.0.8)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic>=0.3->fschat==0.2.36) (1.2.1)\n",
            "Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi->fschat==0.2.36)\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m545.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typer>=0.12.3 (from fastapi-cli>=0.0.2->fastapi->fschat==0.2.36)\n",
            "  Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->fschat==0.2.36) (2.1.5)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.0.0->fschat==0.2.36) (0.1.2)\n",
            "Collecting httptools>=0.5.0 (from uvicorn->fschat==0.2.36)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn->fschat==0.2.36)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn->fschat==0.2.36)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn->fschat==0.2.36)\n",
            "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn->fschat==0.2.36)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray->fschat==0.2.36) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray->fschat==0.2.36) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray->fschat==0.2.36) (0.18.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->fschat==0.2.36) (1.3.0)\n",
            "Collecting svgwrite (from wavedrom->markdown2[all]->fschat==0.2.36)\n",
            "  Downloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from wavedrom->markdown2[all]->fschat==0.2.36) (1.16.0)\n",
            "Collecting shellingham>=1.3.0 (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi->fschat==0.2.36)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Building wheels for collected packages: fschat, wavedrom\n",
            "  Building editable for fschat (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fschat: filename=fschat-0.2.36-0.editable-py3-none-any.whl size=14359 sha256=e7bef98a81a0699380bb3c70f67fe6b80f412ea951b729622053435d1c78f2c7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-c2zyl1c9/wheels/0e/7d/3f/6256e4d259fdebc8665545ea33ca3112027cecb15f3a295311\n",
            "  Building wheel for wavedrom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wavedrom: filename=wavedrom-2.0.3.post3-py2.py3-none-any.whl size=30055 sha256=0784b710247618242c4aaaf7565a8e67a889caa7b73b31dcebee8139595d32cb\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/52/8c/38b454b42f712f325e26f633287484c7dc1ad469e1580c5954\n",
            "Successfully built fschat wavedrom\n",
            "Installing collected packages: nh3, websockets, uvloop, ujson, svgwrite, shortuuid, shellingham, python-multipart, python-dotenv, orjson, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, markdown2, httptools, h11, dnspython, wavedrom, watchfiles, uvicorn, tiktoken, starlette, nvidia-cusparse-cu12, nvidia-cudnn-cu12, httpcore, email_validator, typer, openai, nvidia-cusolver-cu12, httpx, ray, fastapi-cli, anthropic, fastapi, accelerate, peft, fschat\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.9.4\n",
            "    Uninstalling typer-0.9.4:\n",
            "      Successfully uninstalled typer-0.9.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
            "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.30.1 anthropic-0.26.0 dnspython-2.6.1 email_validator-2.1.1 fastapi-0.111.0 fastapi-cli-0.0.4 fschat-0.2.36 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 markdown2-2.4.13 nh3-0.2.17 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 openai-0.28.1 orjson-3.10.3 peft-0.11.1 python-dotenv-1.0.1 python-multipart-0.0.9 ray-2.22.0 shellingham-1.5.4 shortuuid-1.0.13 starlette-0.37.2 svgwrite-1.4.3 tiktoken-0.7.0 typer-0.12.3 ujson-5.10.0 uvicorn-0.29.0 uvloop-0.19.0 watchfiles-0.21.0 wavedrom-2.0.3.post3 websockets-12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/FastChat/fastchat/llm_judge/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2SNLPzuUSgg",
        "outputId": "bce150e3-7930-4a2e-ac46-dbd5c0a60790"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/FastChat/fastchat/llm_judge\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 download_mt_bench_pregenerated.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqGXfFgVwbmf",
        "outputId": "7b2ddc63-7156-4ddd-cc22-226c1b91a7da"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wget -q --show-progress -O data/mt_bench/model_answer/alpaca-13b.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/alpaca-13b.jsonl\n",
            "data/mt_bench/model 100%[===================>]  64.63K  --.-KB/s    in 0.003s  \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/baize-v2-13b.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/baize-v2-13b.jsonl\n",
            "data/mt_bench/model 100%[===================>] 171.91K  --.-KB/s    in 0.005s  \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/chatglm-6b.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/chatglm-6b.jsonl\n",
            "data/mt_bench/model 100%[===================>] 183.80K  --.-KB/s    in 0.006s  \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/claude-instant-v1.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/claude-instant-v1.jsonl\n",
            "data/mt_bench/model 100%[===================>] 161.20K  --.-KB/s    in 0.005s  \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/claude-v1.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/claude-v1.jsonl\n",
            "data/mt_bench/model 100%[===================>] 170.59K  --.-KB/s    in 0.006s  \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/dolly-v2-12b.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/dolly-v2-12b.jsonl\n",
            "data/mt_bench/model 100%[===================>]  88.37K  --.-KB/s    in 0.005s  \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/falcon-40b-instruct.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/falcon-40b-instruct.jsonl\n",
            "data/mt_bench/model 100%[===================>] 101.44K  --.-KB/s    in 0.002s  \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/fastchat-t5-3b.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/fastchat-t5-3b.jsonl\n",
            "data/mt_bench/model 100%[===================>] 175.38K  --.-KB/s    in 0.004s  \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/gpt-3.5-turbo.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/gpt-3.5-turbo.jsonl\n",
            "data/mt_bench/model 100%[===================>] 154.56K  --.-KB/s    in 0.005s  \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/gpt-4.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/gpt-4.jsonl\n",
            "data/mt_bench/model 100%[===================>] 194.38K  --.-KB/s    in 0.008s  \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/gpt4all-13b-snoozy.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/gpt4all-13b-snoozy.jsonl\n",
            "data/mt_bench/model 100%[===================>] 139.48K  --.-KB/s    in 0.005s  \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/guanaco-33b.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/guanaco-33b.jsonl\n",
            "data/mt_bench/model 100%[===================>] 187.41K  --.-KB/s    in 0.004s  \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/guanaco-65b.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/guanaco-65b.jsonl\n",
            "data/mt_bench/model 100%[===================>] 177.82K  --.-KB/s    in 0.005s  \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/h2ogpt-oasst-open-llama-13b.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/h2ogpt-oasst-open-llama-13b.jsonl\n",
            "data/mt_bench/model 100%[===================>] 159.00K  --.-KB/s    in 0.008s  \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/koala-13b.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/koala-13b.jsonl\n",
            "data/mt_bench/model 100%[===================>] 187.34K  --.-KB/s    in 0.005s  \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/llama-13b.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/llama-13b.jsonl\n",
            "data/mt_bench/model 100%[===================>] 126.98K  --.-KB/s    in 0.005s  \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/mpt-30b-chat.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/mpt-30b-chat.jsonl\n",
            "data/mt_bench/model 100%[===================>] 157.30K  --.-KB/s    in 0.004s  \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/mpt-30b-instruct.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/mpt-30b-instruct.jsonl\n",
            "data/mt_bench/model 100%[===================>]  91.42K  --.-KB/s    in 0.003s  \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/mpt-7b-chat.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/mpt-7b-chat.jsonl\n",
            "data/mt_bench/model 100%[===================>] 130.53K  --.-KB/s    in 0.006s  \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/nous-hermes-13b.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/nous-hermes-13b.jsonl\n",
            "data/mt_bench/model 100%[===================>] 115.20K  --.-KB/s    in 0.003s  \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/oasst-sft-4-pythia-12b.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/oasst-sft-4-pythia-12b.jsonl\n",
            "data/mt_bench/model 100%[===================>] 117.77K  --.-KB/s    in 0.005s  \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/oasst-sft-7-llama-30b.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/oasst-sft-7-llama-30b.jsonl\n",
            "data/mt_bench/model 100%[===================>] 135.10K  --.-KB/s    in 0.005s  \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/palm-2-chat-bison-001.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/palm-2-chat-bison-001.jsonl\n",
            "data/mt_bench/model 100%[===================>] 117.59K  --.-KB/s    in 0.006s  \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/rwkv-4-raven-14b.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/rwkv-4-raven-14b.jsonl\n",
            "data/mt_bench/model 100%[===================>] 181.97K   793KB/s    in 0.2s    \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/stablelm-tuned-alpha-7b.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/stablelm-tuned-alpha-7b.jsonl\n",
            "data/mt_bench/model 100%[===================>] 386.35K  --.-KB/s    in 0.01s   \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/tulu-30b.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/tulu-30b.jsonl\n",
            "data/mt_bench/model 100%[===================>] 128.37K  --.-KB/s    in 0.004s  \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/vicuna-13b-v1.3.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/vicuna-13b-v1.3.jsonl\n",
            "data/mt_bench/model 100%[===================>] 182.69K  --.-KB/s    in 0.008s  \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/vicuna-33b-v1.3.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/vicuna-33b-v1.3.jsonl\n",
            "data/mt_bench/model 100%[===================>] 227.01K  --.-KB/s    in 0.006s  \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/vicuna-7b-v1.3.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/vicuna-7b-v1.3.jsonl\n",
            "data/mt_bench/model 100%[===================>] 180.99K  --.-KB/s    in 0.005s  \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/wizardlm-13b.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/wizardlm-13b.jsonl\n",
            "data/mt_bench/model 100%[===================>] 151.79K  --.-KB/s    in 0.005s  \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/wizardlm-30b.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/wizardlm-30b.jsonl\n",
            "data/mt_bench/model 100%[===================>] 157.63K  --.-KB/s    in 0.009s  \n",
            "wget -q --show-progress -O data/mt_bench/model_judgment/gpt-4_single.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_judgment/gpt-4_single.jsonl\n",
            "data/mt_bench/model 100%[===================>]  19.18M  --.-KB/s    in 0.09s   \n",
            "wget -q --show-progress -O data/mt_bench/model_judgment/gpt-4_pair.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_judgment/gpt-4_pair.jsonl\n",
            "data/mt_bench/model 100%[===================>]  45.82M   270MB/s    in 0.2s    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python gen_model_answer.py --model-path lmsys/vicuna-7b-v1.5 --model-id vicuna-7b-v1.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xF1RTBsCxgoE",
        "outputId": "034a7cf9-e49b-44e6-9c22-50e91d29ebad"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output to data/mt_bench/model_answer/vicuna-7b-v1.5.jsonl\n",
            "tokenizer_config.json: 100% 749/749 [00:00<00:00, 4.31MB/s]\n",
            "tokenizer.model: 100% 500k/500k [00:00<00:00, 47.1MB/s]\n",
            "special_tokens_map.json: 100% 438/438 [00:00<00:00, 2.54MB/s]\n",
            "config.json: 100% 615/615 [00:00<00:00, 3.59MB/s]\n",
            "pytorch_model.bin.index.json: 100% 26.8k/26.8k [00:00<00:00, 86.3MB/s]\n",
            "Downloading shards:   0% 0/2 [00:00<?, ?it/s]\n",
            "pytorch_model-00001-of-00002.bin:   0% 0.00/9.98G [00:00<?, ?B/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   0% 41.9M/9.98G [00:00<00:23, 418MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   1% 105M/9.98G [00:00<00:19, 506MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   2% 168M/9.98G [00:00<00:19, 515MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   2% 231M/9.98G [00:00<00:18, 531MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   3% 294M/9.98G [00:00<00:17, 549MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   4% 357M/9.98G [00:00<00:17, 556MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   4% 419M/9.98G [00:00<00:17, 548MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   5% 482M/9.98G [00:00<00:18, 509MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   5% 535M/9.98G [00:01<00:18, 513MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   6% 587M/9.98G [00:01<00:19, 487MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   6% 640M/9.98G [00:01<00:19, 491MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   7% 692M/9.98G [00:01<00:20, 457MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   7% 744M/9.98G [00:01<00:20, 457MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   8% 797M/9.98G [00:01<00:22, 414MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   9% 849M/9.98G [00:01<00:24, 369MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   9% 902M/9.98G [00:01<00:23, 393MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  10% 954M/9.98G [00:02<00:22, 409MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  10% 1.01G/9.98G [00:02<00:21, 422MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.06G/9.98G [00:02<00:20, 436MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.11G/9.98G [00:02<00:19, 447MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.16G/9.98G [00:02<00:19, 456MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.22G/9.98G [00:02<00:19, 461MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.27G/9.98G [00:02<00:18, 469MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.32G/9.98G [00:02<00:18, 466MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.37G/9.98G [00:03<00:22, 391MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.43G/9.98G [00:03<00:21, 405MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.48G/9.98G [00:03<00:21, 404MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.53G/9.98G [00:03<00:21, 393MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  16% 1.58G/9.98G [00:03<00:20, 419MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  16% 1.64G/9.98G [00:03<00:19, 434MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.70G/9.98G [00:03<00:17, 470MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.76G/9.98G [00:03<00:16, 498MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.82G/9.98G [00:03<00:15, 512MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.89G/9.98G [00:04<00:15, 528MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  20% 1.95G/9.98G [00:04<00:14, 536MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  20% 2.01G/9.98G [00:04<00:15, 526MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.08G/9.98G [00:04<00:15, 521MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.13G/9.98G [00:04<00:16, 485MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.18G/9.98G [00:04<00:16, 475MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.23G/9.98G [00:04<00:16, 482MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.30G/9.98G [00:04<00:15, 499MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.35G/9.98G [00:05<00:18, 404MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.40G/9.98G [00:05<00:25, 292MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.44G/9.98G [00:05<00:31, 242MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.47G/9.98G [00:05<00:35, 209MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.51G/9.98G [00:06<00:40, 184MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.54G/9.98G [00:06<00:47, 158MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.56G/9.98G [00:06<00:52, 141MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.58G/9.98G [00:06<00:51, 143MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.60G/9.98G [00:06<00:48, 153MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.66G/9.98G [00:07<00:30, 239MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.72G/9.98G [00:07<00:24, 294MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.77G/9.98G [00:07<00:21, 338MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.82G/9.98G [00:07<00:19, 373MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.87G/9.98G [00:07<00:17, 400MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.93G/9.98G [00:07<00:16, 422MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  30% 2.98G/9.98G [00:07<00:16, 430MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  30% 3.03G/9.98G [00:07<00:15, 435MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.08G/9.98G [00:07<00:15, 447MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.14G/9.98G [00:07<00:14, 467MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.19G/9.98G [00:08<00:14, 482MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.24G/9.98G [00:08<00:13, 490MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.29G/9.98G [00:08<00:13, 494MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.34G/9.98G [00:08<00:14, 472MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.40G/9.98G [00:08<00:14, 467MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  35% 3.45G/9.98G [00:08<00:13, 477MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  35% 3.50G/9.98G [00:08<00:13, 482MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.57G/9.98G [00:08<00:12, 500MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.62G/9.98G [00:08<00:13, 476MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  37% 3.67G/9.98G [00:09<00:14, 441MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  37% 3.72G/9.98G [00:09<00:14, 438MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.77G/9.98G [00:09<00:14, 432MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.83G/9.98G [00:09<00:14, 435MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.88G/9.98G [00:09<00:14, 431MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.93G/9.98G [00:12<01:37, 61.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  40% 4.00G/9.98G [00:12<01:07, 88.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.05G/9.98G [00:12<00:51, 116MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.11G/9.98G [00:12<00:37, 156MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.17G/9.98G [00:12<00:28, 202MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.24G/9.98G [00:12<00:22, 252MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.30G/9.98G [00:12<00:18, 304MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.36G/9.98G [00:12<00:15, 351MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.42G/9.98G [00:13<00:14, 388MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  45% 4.48G/9.98G [00:13<00:14, 391MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  45% 4.53G/9.98G [00:13<00:13, 397MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.58G/9.98G [00:13<00:13, 399MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.63G/9.98G [00:13<00:13, 408MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.69G/9.98G [00:13<00:14, 361MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.73G/9.98G [00:13<00:16, 325MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.77G/9.98G [00:14<00:15, 345MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.82G/9.98G [00:14<00:13, 374MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.88G/9.98G [00:14<00:12, 396MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.93G/9.98G [00:14<00:12, 418MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  50% 4.98G/9.98G [00:14<00:12, 412MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  50% 5.03G/9.98G [00:14<00:11, 424MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  51% 5.09G/9.98G [00:14<00:11, 438MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.14G/9.98G [00:14<00:10, 447MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.19G/9.98G [00:14<00:10, 453MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.24G/9.98G [00:15<00:10, 455MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.30G/9.98G [00:15<00:10, 459MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.35G/9.98G [00:15<00:10, 461MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.40G/9.98G [00:15<00:09, 462MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  55% 5.45G/9.98G [00:15<00:09, 464MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  55% 5.51G/9.98G [00:15<00:10, 433MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.56G/9.98G [00:15<00:10, 412MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.60G/9.98G [00:15<00:11, 391MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.64G/9.98G [00:16<00:11, 394MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.68G/9.98G [00:16<00:11, 387MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.74G/9.98G [00:16<00:10, 407MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  58% 5.79G/9.98G [00:16<00:09, 428MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.84G/9.98G [00:16<00:10, 388MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.89G/9.98G [00:16<00:10, 404MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  60% 5.95G/9.98G [00:16<00:09, 416MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  60% 6.00G/9.98G [00:16<00:09, 426MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.05G/9.98G [00:17<00:09, 433MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.10G/9.98G [00:17<00:08, 435MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.16G/9.98G [00:17<00:08, 434MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.21G/9.98G [00:17<00:08, 438MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.26G/9.98G [00:17<00:08, 442MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.31G/9.98G [00:17<00:08, 444MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  64% 6.36G/9.98G [00:17<00:08, 447MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  64% 6.42G/9.98G [00:17<00:07, 448MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.47G/9.98G [00:17<00:07, 450MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.52G/9.98G [00:18<00:07, 450MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.57G/9.98G [00:18<00:07, 449MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.63G/9.98G [00:18<00:07, 451MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.68G/9.98G [00:18<00:07, 450MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.73G/9.98G [00:18<00:07, 449MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.78G/9.98G [00:18<00:07, 447MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.84G/9.98G [00:18<00:07, 448MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.89G/9.98G [00:18<00:06, 447MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  70% 6.94G/9.98G [00:19<00:06, 441MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  70% 6.99G/9.98G [00:19<00:06, 438MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.05G/9.98G [00:19<00:06, 435MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.10G/9.98G [00:19<00:06, 436MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.15G/9.98G [00:19<00:06, 429MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.20G/9.98G [00:19<00:06, 425MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.26G/9.98G [00:19<00:06, 422MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.31G/9.98G [00:19<00:06, 426MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.36G/9.98G [00:20<00:06, 434MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.41G/9.98G [00:20<00:05, 437MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.47G/9.98G [00:20<00:05, 441MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.52G/9.98G [00:20<00:05, 445MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.57G/9.98G [00:20<00:05, 441MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.62G/9.98G [00:20<00:05, 443MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.68G/9.98G [00:20<00:05, 448MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.73G/9.98G [00:20<00:05, 449MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.78G/9.98G [00:20<00:04, 450MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.83G/9.98G [00:21<00:04, 452MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.89G/9.98G [00:21<00:04, 452MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  80% 7.94G/9.98G [00:21<00:04, 453MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  80% 7.99G/9.98G [00:21<00:04, 450MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.04G/9.98G [00:23<00:26, 72.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.10G/9.98G [00:23<00:19, 97.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.15G/9.98G [00:23<00:14, 128MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.20G/9.98G [00:23<00:10, 163MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.25G/9.98G [00:23<00:08, 204MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.30G/9.98G [00:24<00:06, 248MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.36G/9.98G [00:24<00:05, 292MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.41G/9.98G [00:24<00:04, 336MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.46G/9.98G [00:24<00:04, 373MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.51G/9.98G [00:24<00:03, 406MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.57G/9.98G [00:24<00:03, 432MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.62G/9.98G [00:24<00:02, 456MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.67G/9.98G [00:24<00:02, 469MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.72G/9.98G [00:24<00:02, 479MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.78G/9.98G [00:25<00:03, 310MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.83G/9.98G [00:25<00:03, 348MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  89% 8.88G/9.98G [00:25<00:02, 380MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  90% 8.93G/9.98G [00:25<00:02, 406MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  90% 8.99G/9.98G [00:25<00:02, 421MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.04G/9.98G [00:25<00:02, 437MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.09G/9.98G [00:25<00:01, 453MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.14G/9.98G [00:25<00:01, 467MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.20G/9.98G [00:26<00:01, 481MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.25G/9.98G [00:26<00:01, 490MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.30G/9.98G [00:26<00:01, 490MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.35G/9.98G [00:26<00:01, 495MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.41G/9.98G [00:26<00:01, 499MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.46G/9.98G [00:26<00:01, 493MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.51G/9.98G [00:26<00:00, 499MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.56G/9.98G [00:26<00:00, 477MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.62G/9.98G [00:26<00:00, 454MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.67G/9.98G [00:27<00:00, 464MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.72G/9.98G [00:27<00:00, 473MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.77G/9.98G [00:27<00:00, 483MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.83G/9.98G [00:27<00:00, 447MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  99% 9.88G/9.98G [00:27<00:00, 424MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin: 100% 9.93G/9.98G [00:27<00:00, 428MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin: 100% 9.98G/9.98G [00:27<00:00, 359MB/s]\n",
            "Downloading shards:  50% 1/2 [00:28<00:28, 28.08s/it]\n",
            "pytorch_model-00002-of-00002.bin:   0% 0.00/3.50G [00:00<?, ?B/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   1% 41.9M/3.50G [00:00<00:08, 396MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   3% 94.4M/3.50G [00:00<00:07, 461MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   4% 157M/3.50G [00:00<00:06, 495MB/s] \u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   6% 210M/3.50G [00:00<00:06, 497MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   7% 262M/3.50G [00:00<00:07, 455MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   9% 315M/3.50G [00:00<00:06, 456MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  11% 377M/3.50G [00:00<00:06, 485MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  12% 430M/3.50G [00:00<00:06, 491MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  14% 482M/3.50G [00:01<00:06, 489MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  15% 535M/3.50G [00:01<00:06, 489MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  17% 587M/3.50G [00:01<00:05, 491MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  19% 650M/3.50G [00:01<00:05, 505MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  20% 713M/3.50G [00:01<00:05, 515MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  22% 765M/3.50G [00:01<00:05, 470MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  24% 828M/3.50G [00:01<00:05, 487MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  25% 881M/3.50G [00:01<00:05, 462MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  27% 933M/3.50G [00:01<00:05, 470MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  28% 996M/3.50G [00:02<00:05, 491MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  30% 1.06G/3.50G [00:02<00:04, 502MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  32% 1.11G/3.50G [00:02<00:05, 477MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  33% 1.16G/3.50G [00:02<00:05, 435MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  35% 1.22G/3.50G [00:02<00:05, 432MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  36% 1.27G/3.50G [00:02<00:05, 419MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  38% 1.32G/3.50G [00:02<00:05, 408MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  39% 1.37G/3.50G [00:02<00:05, 419MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  41% 1.43G/3.50G [00:03<00:04, 445MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  43% 1.49G/3.50G [00:03<00:04, 472MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  44% 1.54G/3.50G [00:03<00:04, 466MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  46% 1.59G/3.50G [00:03<00:04, 471MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  47% 1.65G/3.50G [00:03<00:03, 475MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  49% 1.70G/3.50G [00:03<00:04, 443MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  50% 1.75G/3.50G [00:03<00:04, 424MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  52% 1.80G/3.50G [00:03<00:04, 420MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  53% 1.86G/3.50G [00:04<00:04, 368MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  54% 1.90G/3.50G [00:04<00:04, 368MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  56% 1.96G/3.50G [00:04<00:03, 412MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  58% 2.01G/3.50G [00:04<00:03, 419MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  59% 2.07G/3.50G [00:04<00:03, 423MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  61% 2.12G/3.50G [00:04<00:03, 438MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  62% 2.17G/3.50G [00:04<00:03, 373MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  64% 2.22G/3.50G [00:04<00:03, 390MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  65% 2.29G/3.50G [00:05<00:02, 428MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  67% 2.34G/3.50G [00:05<00:02, 443MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  68% 2.39G/3.50G [00:05<00:02, 430MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  70% 2.44G/3.50G [00:05<00:02, 407MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  71% 2.50G/3.50G [00:05<00:02, 435MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  73% 2.56G/3.50G [00:05<00:01, 473MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  75% 2.61G/3.50G [00:05<00:01, 478MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  76% 2.66G/3.50G [00:05<00:01, 476MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  78% 2.72G/3.50G [00:06<00:01, 473MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  79% 2.77G/3.50G [00:06<00:01, 462MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  81% 2.82G/3.50G [00:06<00:01, 416MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  82% 2.87G/3.50G [00:06<00:01, 391MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  83% 2.92G/3.50G [00:06<00:01, 369MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  84% 2.96G/3.50G [00:06<00:01, 361MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  86% 3.00G/3.50G [00:06<00:01, 353MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  87% 3.04G/3.50G [00:06<00:01, 345MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  88% 3.08G/3.50G [00:07<00:01, 337MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  89% 3.12G/3.50G [00:07<00:01, 331MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  90% 3.17G/3.50G [00:07<00:01, 320MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  92% 3.21G/3.50G [00:07<00:00, 304MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  93% 3.24G/3.50G [00:07<00:00, 295MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  93% 3.27G/3.50G [00:07<00:00, 289MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  94% 3.30G/3.50G [00:07<00:00, 283MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  95% 3.33G/3.50G [00:08<00:00, 273MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  96% 3.37G/3.50G [00:08<00:00, 262MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  97% 3.40G/3.50G [00:08<00:00, 247MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  98% 3.43G/3.50G [00:08<00:00, 222MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  99% 3.46G/3.50G [00:08<00:00, 202MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  99% 3.48G/3.50G [00:08<00:00, 186MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin: 100% 3.50G/3.50G [00:09<00:00, 386MB/s]\n",
            "Downloading shards: 100% 2/2 [00:37<00:00, 18.70s/it]\n",
            "Loading checkpoint shards: 100% 2/2 [00:00<00:00,  6.69it/s]\n",
            "generation_config.json: 100% 162/162 [00:00<00:00, 841kB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "  4% 3/80 [01:38<36:32, 28.48s/it]  /usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "100% 80/80 [25:08<00:00, 18.85s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%env OPENAI_API_KEY=your_openai_api_key"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Qni4b0Rkox0",
        "outputId": "03446f40-cc45-452e-d051-dc7a6d0ca3fd"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: OPENAI_API_KEY=your_openai_api_key\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python gen_judgment.py --model-list vicuna-7b-v1.5 --parallel 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Q-eHuSxx723",
        "outputId": "45666411-02f1-4eec-eb23-a50148b18c84"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stats:\n",
            "{\n",
            "    \"bench_name\": \"mt_bench\",\n",
            "    \"mode\": \"single\",\n",
            "    \"judge\": \"gpt-4\",\n",
            "    \"baseline\": null,\n",
            "    \"model_list\": [\n",
            "        \"vicuna-7b-v1.5\"\n",
            "    ],\n",
            "    \"total_num_questions\": 80,\n",
            "    \"total_num_matches\": 160,\n",
            "    \"output_path\": \"data/mt_bench/model_judgment/gpt-4_single.jsonl\"\n",
            "}\n",
            "Press Enter to confirm...\n",
            "  0% 0/160 [00:00<?, ?it/s]question: 141, turn: 2, model: vicuna-7b-v1.5, score: 8, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "  1% 1/160 [00:05<15:44,  5.94s/it]question: 143, turn: 2, model: vicuna-7b-v1.5, score: 6, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "  1% 2/160 [00:06<07:28,  2.84s/it]question: 88, turn: 1, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-v1')\n",
            "question: 155, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1')\n",
            "question: 132, turn: 2, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "question: 114, turn: 2, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            "  2% 3/160 [00:27<29:14, 11.17s/it]question: 153, turn: 2, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "  4% 7/160 [00:28<08:11,  3.21s/it]question: 86, turn: 2, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "question: 87, turn: 2, model: vicuna-7b-v1.5, score: 8, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "question: 104, turn: 2, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            "question: 117, turn: 1, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-math-v1')\n",
            "  5% 8/160 [00:44<15:03,  5.94s/it]question: 93, turn: 2, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "  8% 12/160 [00:46<07:23,  3.00s/it]question: 137, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1')\n",
            "  8% 13/160 [00:50<07:25,  3.03s/it]question: 150, turn: 2, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "question: 117, turn: 2, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            "  9% 14/160 [01:04<12:26,  5.11s/it]question: 113, turn: 1, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-math-v1')\n",
            " 10% 16/160 [01:05<08:21,  3.48s/it]question: 98, turn: 2, model: vicuna-7b-v1.5, score: 8, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "question: 102, turn: 1, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-math-v1')\n",
            " 11% 17/160 [01:10<09:19,  3.91s/it]<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-240KpuUeMDxI7SRQWzHZqWHV on tokens per min (TPM): Limit 10000, Used 7430, Requested 2707. Please try again in 822ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 159, turn: 2, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            " 12% 19/160 [01:14<07:36,  3.24s/it]question: 156, turn: 1, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-v1')\n",
            "question: 107, turn: 1, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-math-v1')\n",
            "question: 91, turn: 2, model: vicuna-7b-v1.5, score: 6, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            " 12% 20/160 [01:26<11:39,  5.00s/it]question: 110, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-math-v1')\n",
            "<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-240KpuUeMDxI7SRQWzHZqWHV on tokens per min (TPM): Limit 10000, Used 7086, Requested 2995. Please try again in 486ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 89, turn: 1, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-v1')\n",
            " 15% 24/160 [01:31<06:41,  2.95s/it]question: 144, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1')\n",
            "question: 148, turn: 1, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-v1')\n",
            " 16% 25/160 [01:42<09:34,  4.26s/it]question: 112, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-math-v1')\n",
            "question: 118, turn: 2, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            " 17% 27/160 [01:48<08:44,  3.94s/it]question: 99, turn: 2, model: vicuna-7b-v1.5, score: 4, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            " 18% 29/160 [01:51<06:41,  3.07s/it]<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-240KpuUeMDxI7SRQWzHZqWHV on tokens per min (TPM): Limit 10000, Used 7410, Requested 2903. Please try again in 1.878s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 152, turn: 2, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            " 19% 30/160 [01:53<06:31,  3.01s/it]question: 135, turn: 1, model: vicuna-7b-v1.5, score: 8, judge: ('gpt-4', 'single-v1')\n",
            "question: 106, turn: 2, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            " 19% 31/160 [02:08<11:42,  5.45s/it]<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-240KpuUeMDxI7SRQWzHZqWHV on tokens per min (TPM): Limit 10000, Used 6427, Requested 3842. Please try again in 1.614s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 105, turn: 1, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-math-v1')\n",
            " 21% 33/160 [02:09<07:25,  3.51s/it]question: 100, turn: 1, model: vicuna-7b-v1.5, score: 8, judge: ('gpt-4', 'single-v1')\n",
            "<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-240KpuUeMDxI7SRQWzHZqWHV on tokens per min (TPM): Limit 10000, Used 6171, Requested 3842. Please try again in 78ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 125, turn: 1, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-math-v1')\n",
            "question: 81, turn: 2, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            " 21% 34/160 [02:34<17:00,  8.10s/it]question: 102, turn: 2, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            " 23% 37/160 [02:35<09:07,  4.45s/it]question: 97, turn: 1, model: vicuna-7b-v1.5, score: 4, judge: ('gpt-4', 'single-v1')\n",
            " 24% 38/160 [02:40<09:04,  4.46s/it]question: 151, turn: 2, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "question: 111, turn: 1, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-math-v1')\n",
            " 24% 39/160 [02:50<11:35,  5.75s/it]question: 151, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1')\n",
            " 26% 41/160 [02:52<07:45,  3.91s/it]<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-240KpuUeMDxI7SRQWzHZqWHV on tokens per min (TPM): Limit 10000, Used 7350, Requested 2669. Please try again in 114ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 133, turn: 1, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-v1')\n",
            " 26% 42/160 [02:55<07:13,  3.67s/it]question: 122, turn: 2, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            "<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-240KpuUeMDxI7SRQWzHZqWHV on tokens per min (TPM): Limit 10000, Used 7111, Requested 2924. Please try again in 210ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 114, turn: 1, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-math-v1')\n",
            " 27% 43/160 [03:16<15:12,  7.80s/it]question: 142, turn: 2, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            " 28% 45/160 [03:24<11:55,  6.22s/it]question: 116, turn: 2, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            " 29% 46/160 [03:29<11:05,  5.84s/it]question: 107, turn: 2, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            " 29% 47/160 [03:30<09:00,  4.78s/it]<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-240KpuUeMDxI7SRQWzHZqWHV on tokens per min (TPM): Limit 10000, Used 7195, Requested 3265. Please try again in 2.76s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 138, turn: 1, model: vicuna-7b-v1.5, score: 8, judge: ('gpt-4', 'single-v1')\n",
            " 30% 48/160 [03:34<08:24,  4.50s/it]question: 96, turn: 2, model: vicuna-7b-v1.5, score: 8, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "question: 149, turn: 2, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            " 31% 49/160 [03:45<11:50,  6.40s/it]question: 106, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-math-v1')\n",
            " 32% 51/160 [03:46<06:50,  3.76s/it]question: 140, turn: 2, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            " 32% 52/160 [03:50<07:00,  3.89s/it]<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-240KpuUeMDxI7SRQWzHZqWHV on tokens per min (TPM): Limit 10000, Used 7518, Requested 4006. Please try again in 9.144s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 99, turn: 1, model: vicuna-7b-v1.5, score: 5, judge: ('gpt-4', 'single-v1')\n",
            " 33% 53/160 [03:53<06:18,  3.53s/it]question: 141, turn: 1, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-v1')\n",
            "<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-240KpuUeMDxI7SRQWzHZqWHV on tokens per min (TPM): Limit 10000, Used 6870, Requested 4006. Please try again in 5.256s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 95, turn: 2, model: vicuna-7b-v1.5, score: 5, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "question: 124, turn: 1, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-math-v1')\n",
            "question: 138, turn: 2, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "question: 83, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1')\n",
            "question: 128, turn: 2, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            " 34% 54/160 [04:36<24:41, 13.98s/it]<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-240KpuUeMDxI7SRQWzHZqWHV on tokens per min (TPM): Limit 10000, Used 7724, Requested 3366. Please try again in 6.54s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 110, turn: 2, model: vicuna-7b-v1.5, score: 7, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            " 38% 60/160 [04:50<09:55,  5.96s/it]question: 84, turn: 2, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            " 38% 61/160 [04:51<08:33,  5.18s/it]<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-240KpuUeMDxI7SRQWzHZqWHV on tokens per min (TPM): Limit 10000, Used 7676, Requested 2727. Please try again in 2.418s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 154, turn: 1, model: vicuna-7b-v1.5, score: 8, judge: ('gpt-4', 'single-v1')\n",
            " 39% 62/160 [04:58<08:59,  5.50s/it]question: 91, turn: 1, model: vicuna-7b-v1.5, score: 8, judge: ('gpt-4', 'single-v1')\n",
            "question: 112, turn: 2, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            "<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-240KpuUeMDxI7SRQWzHZqWHV on tokens per min (TPM): Limit 10000, Used 7039, Requested 3607. Please try again in 3.876s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 122, turn: 1, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-math-v1')\n",
            " 39% 63/160 [05:19<13:44,  8.50s/it]question: 97, turn: 2, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "question: 90, turn: 2, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "question: 125, turn: 2, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            " 41% 66/160 [05:33<10:34,  6.75s/it]<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-240KpuUeMDxI7SRQWzHZqWHV on tokens per min (TPM): Limit 10000, Used 6891, Requested 3665. Please try again in 3.336s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 115, turn: 2, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            " 43% 69/160 [05:36<06:57,  4.58s/it]question: 155, turn: 2, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-240KpuUeMDxI7SRQWzHZqWHV on tokens per min (TPM): Limit 10000, Used 6504, Requested 3665. Please try again in 1.013s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 101, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-math-v1')\n",
            "question: 131, turn: 2, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-240KpuUeMDxI7SRQWzHZqWHV on tokens per min (TPM): Limit 10000, Used 6867, Requested 3665. Please try again in 3.192s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 113, turn: 2, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            "question: 115, turn: 1, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-math-v1')\n",
            "question: 137, turn: 2, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "question: 103, turn: 2, model: vicuna-7b-v1.5, score: 7, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            " 44% 70/160 [06:23<16:50, 11.23s/it]question: 139, turn: 2, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            " 48% 77/160 [06:26<06:22,  4.61s/it]question: 120, turn: 1, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-math-v1')\n",
            " 49% 78/160 [06:31<06:22,  4.67s/it]question: 160, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1')\n",
            " 49% 79/160 [06:33<05:47,  4.29s/it]question: 159, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1')\n",
            " 50% 80/160 [06:36<05:31,  4.14s/it]<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-240KpuUeMDxI7SRQWzHZqWHV on tokens per min (TPM): Limit 10000, Used 6386, Requested 3678. Please try again in 384ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 94, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1')\n",
            " 51% 81/160 [06:39<05:06,  3.88s/it]question: 134, turn: 1, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-v1')\n",
            "question: 144, turn: 2, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "question: 131, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1')\n",
            "question: 96, turn: 1, model: vicuna-7b-v1.5, score: 8, judge: ('gpt-4', 'single-v1')\n",
            "question: 129, turn: 2, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            " 51% 82/160 [07:05<11:13,  8.64s/it]question: 103, turn: 1, model: vicuna-7b-v1.5, score: 6, judge: ('gpt-4', 'single-math-v1')\n",
            "question: 129, turn: 1, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-math-v1')\n",
            " 54% 87/160 [07:25<07:03,  5.80s/it]question: 84, turn: 1, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-v1')\n",
            "question: 127, turn: 1, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-math-v1')\n",
            " 56% 89/160 [07:40<07:25,  6.27s/it]question: 130, turn: 2, model: vicuna-7b-v1.5, score: 3, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            " 57% 91/160 [07:46<06:13,  5.41s/it]question: 94, turn: 2, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            " 57% 92/160 [07:47<05:11,  4.59s/it]question: 87, turn: 1, model: vicuna-7b-v1.5, score: 8, judge: ('gpt-4', 'single-v1')\n",
            " 58% 93/160 [07:54<05:37,  5.03s/it]question: 119, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-math-v1')\n",
            " 59% 94/160 [07:57<05:13,  4.76s/it]question: 85, turn: 2, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "question: 160, turn: 2, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "question: 93, turn: 1, model: vicuna-7b-v1.5, score: 6, judge: ('gpt-4', 'single-v1')\n",
            "question: 126, turn: 1, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-math-v1')\n",
            " 59% 95/160 [08:16<08:32,  7.89s/it]<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-240KpuUeMDxI7SRQWzHZqWHV on tokens per min (TPM): Limit 10000, Used 8162, Requested 2412. Please try again in 3.444s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 154, turn: 2, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            " 62% 99/160 [08:22<04:20,  4.27s/it]<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-240KpuUeMDxI7SRQWzHZqWHV on tokens per min (TPM): Limit 10000, Used 7755, Requested 2412. Please try again in 1.002s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 156, turn: 2, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "question: 92, turn: 2, model: vicuna-7b-v1.5, score: 8, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "question: 108, turn: 2, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            "question: 95, turn: 1, model: vicuna-7b-v1.5, score: 7, judge: ('gpt-4', 'single-v1')\n",
            " 62% 100/160 [08:43<07:08,  7.14s/it]question: 157, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1')\n",
            " 65% 104/160 [08:45<03:33,  3.80s/it]<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-240KpuUeMDxI7SRQWzHZqWHV on tokens per min (TPM): Limit 10000, Used 7235, Requested 2842. Please try again in 462ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 92, turn: 1, model: vicuna-7b-v1.5, score: 8, judge: ('gpt-4', 'single-v1')\n",
            " 66% 105/160 [08:49<03:34,  3.90s/it]question: 146, turn: 1, model: vicuna-7b-v1.5, score: 8, judge: ('gpt-4', 'single-v1')\n",
            "question: 133, turn: 2, model: vicuna-7b-v1.5, score: 6, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            " 66% 106/160 [09:00<04:34,  5.09s/it]question: 108, turn: 1, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-math-v1')\n",
            " 68% 108/160 [09:02<03:16,  3.78s/it]<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-240KpuUeMDxI7SRQWzHZqWHV on tokens per min (TPM): Limit 10000, Used 6926, Requested 3834. Please try again in 4.56s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 152, turn: 1, model: vicuna-7b-v1.5, score: 8, judge: ('gpt-4', 'single-v1')\n",
            " 68% 109/160 [09:05<03:04,  3.62s/it]question: 116, turn: 1, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-math-v1')\n",
            "question: 82, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1')\n",
            "question: 101, turn: 2, model: vicuna-7b-v1.5, score: 3, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            "question: 127, turn: 2, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            " 69% 110/160 [09:39<08:34, 10.29s/it]question: 153, turn: 1, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-v1')\n",
            "question: 119, turn: 2, model: vicuna-7b-v1.5, score: 3, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            " 71% 114/160 [09:47<04:22,  5.70s/it]question: 136, turn: 2, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            " 72% 116/160 [09:51<03:20,  4.56s/it]question: 85, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1')\n",
            " 73% 117/160 [09:53<02:56,  4.10s/it]question: 98, turn: 1, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-v1')\n",
            "question: 109, turn: 2, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            " 74% 118/160 [10:02<03:33,  5.08s/it]question: 147, turn: 2, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            " 75% 120/160 [10:05<02:35,  3.89s/it]question: 135, turn: 2, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            " 76% 121/160 [10:07<02:12,  3.39s/it]question: 149, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1')\n",
            " 76% 122/160 [10:11<02:16,  3.60s/it]question: 86, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1')\n",
            " 77% 123/160 [10:11<01:44,  2.83s/it]<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-240KpuUeMDxI7SRQWzHZqWHV on tokens per min (TPM): Limit 10000, Used 7240, Requested 3782. Please try again in 6.132s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 104, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-math-v1')\n",
            " 78% 124/160 [10:16<01:55,  3.21s/it]question: 157, turn: 2, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-240KpuUeMDxI7SRQWzHZqWHV on tokens per min (TPM): Limit 10000, Used 7955, Requested 3782. Please try again in 10.422s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 81, turn: 1, model: vicuna-7b-v1.5, score: 8, judge: ('gpt-4', 'single-v1')\n",
            "<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-240KpuUeMDxI7SRQWzHZqWHV on tokens per min (TPM): Limit 10000, Used 6850, Requested 3782. Please try again in 3.792s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 145, turn: 1, model: vicuna-7b-v1.5, score: 8.5, judge: ('gpt-4', 'single-v1')\n",
            "question: 139, turn: 1, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-v1')\n",
            "<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-240KpuUeMDxI7SRQWzHZqWHV on tokens per min (TPM): Limit 10000, Used 6762, Requested 3782. Please try again in 3.264s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 145, turn: 2, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "question: 142, turn: 1, model: vicuna-7b-v1.5, score: 3, judge: ('gpt-4', 'single-v1')\n",
            "question: 124, turn: 2, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            " 78% 125/160 [10:59<08:09, 13.99s/it]<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-240KpuUeMDxI7SRQWzHZqWHV on tokens per min (TPM): Limit 10000, Used 7552, Requested 3303. Please try again in 5.13s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 105, turn: 2, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            " 82% 132/160 [11:02<01:56,  4.14s/it]<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-240KpuUeMDxI7SRQWzHZqWHV on tokens per min (TPM): Limit 10000, Used 6722, Requested 3303. Please try again in 150ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 158, turn: 2, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            " 83% 133/160 [11:24<02:54,  6.46s/it]question: 120, turn: 2, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            " 84% 134/160 [11:26<02:26,  5.64s/it]question: 143, turn: 1, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-v1')\n",
            " 84% 135/160 [11:30<02:15,  5.44s/it]question: 140, turn: 1, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-v1')\n",
            " 85% 136/160 [11:30<01:45,  4.38s/it]question: 100, turn: 2, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            " 86% 137/160 [11:35<01:42,  4.45s/it]question: 83, turn: 2, model: vicuna-7b-v1.5, score: 7, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            " 86% 138/160 [11:36<01:19,  3.60s/it]<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-240KpuUeMDxI7SRQWzHZqWHV on tokens per min (TPM): Limit 10000, Used 7636, Requested 3131. Please try again in 4.602s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 130, turn: 1, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-math-v1')\n",
            " 87% 139/160 [11:49<02:04,  5.95s/it]<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-240KpuUeMDxI7SRQWzHZqWHV on tokens per min (TPM): Limit 10000, Used 6884, Requested 3677. Please try again in 3.366s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 146, turn: 2, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            " 88% 140/160 [11:52<01:46,  5.32s/it]question: 123, turn: 1, model: vicuna-7b-v1.5, score: 8, judge: ('gpt-4', 'single-math-v1')\n",
            "question: 126, turn: 2, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            " 88% 141/160 [12:11<02:50,  8.95s/it]question: 136, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1')\n",
            "question: 82, turn: 2, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-240KpuUeMDxI7SRQWzHZqWHV on tokens per min (TPM): Limit 10000, Used 6559, Requested 3777. Please try again in 2.015s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 128, turn: 1, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-math-v1')\n",
            " 89% 143/160 [12:25<02:19,  8.22s/it]<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-240KpuUeMDxI7SRQWzHZqWHV on tokens per min (TPM): Limit 10000, Used 6907, Requested 3777. Please try again in 4.104s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 123, turn: 2, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            "question: 150, turn: 1, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-v1')\n",
            "question: 121, turn: 2, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            " 91% 146/160 [12:58<02:13,  9.53s/it]question: 109, turn: 1, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-math-v1')\n",
            " 93% 149/160 [13:04<01:11,  6.47s/it]question: 111, turn: 2, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            " 94% 150/160 [13:07<00:57,  5.79s/it]question: 89, turn: 2, model: vicuna-7b-v1.5, score: 4, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            " 94% 151/160 [13:15<00:56,  6.30s/it]question: 121, turn: 1, model: vicuna-7b-v1.5, score: 7, judge: ('gpt-4', 'single-math-v1')\n",
            " 95% 152/160 [13:21<00:50,  6.30s/it]question: 88, turn: 2, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "question: 132, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1')\n",
            "question: 147, turn: 1, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-v1')\n",
            " 96% 154/160 [13:26<00:28,  4.80s/it]question: 90, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1')\n",
            " 98% 156/160 [13:30<00:15,  3.76s/it]question: 134, turn: 2, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            " 98% 157/160 [13:30<00:09,  3.16s/it]question: 148, turn: 2, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "question: 158, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1')\n",
            "question: 118, turn: 1, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-math-v1')\n",
            "100% 160/160 [13:44<00:00,  5.15s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python show_result.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eWx9pBwMZtP",
        "outputId": "5073031a-800e-4689-bcf7-35412a055b37"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mode: single\n",
            "Input file: data/mt_bench/model_judgment/gpt-4_single.jsonl\n",
            "\n",
            "########## First turn ##########\n",
            "                                    score\n",
            "model                       turn         \n",
            "gpt-4                       1     8.95625\n",
            "claude-v1                   1     8.15000\n",
            "gpt-3.5-turbo               1     8.07500\n",
            "claude-instant-v1           1     7.80000\n",
            "vicuna-33b-v1.3             1     7.45625\n",
            "wizardlm-30b                1     7.13125\n",
            "wizardlm-13b                1     7.11875\n",
            "oasst-sft-7-llama-30b       1     7.10625\n",
            "Llama-2-13b-chat            1     7.06250\n",
            "tulu-30b                    1     7.01875\n",
            "Llama-2-70b-chat            1     6.98750\n",
            "guanaco-33b                 1     6.88125\n",
            "vicuna-13b-v1.3             1     6.81250\n",
            "guanaco-65b                 1     6.78125\n",
            "palm-2-chat-bison-001       1     6.71250\n",
            "vicuna-7b-v1.3              1     6.69375\n",
            "mpt-30b-chat                1     6.67500\n",
            "vicuna-7b-v1.5              1     6.58125\n",
            "nous-hermes-13b             1     6.43125\n",
            "Llama-2-7b-chat             1     6.41250\n",
            "baize-v2-13b                1     6.31875\n",
            "gpt4all-13b-snoozy          1     6.07500\n",
            "koala-13b                   1     6.07500\n",
            "mpt-7b-chat                 1     5.85000\n",
            "falcon-40b-instruct         1     5.81250\n",
            "mpt-30b-instruct            1     5.67500\n",
            "h2ogpt-oasst-open-llama-13b 1     5.51250\n",
            "chatglm-6b                  1     5.00000\n",
            "oasst-sft-4-pythia-12b      1     4.97500\n",
            "alpaca-13b                  1     4.97500\n",
            "rwkv-4-raven-14b            1     4.74375\n",
            "dolly-v2-12b                1     3.80000\n",
            "fastchat-t5-3b              1     3.39375\n",
            "llama-13b                   1     3.26250\n",
            "stablelm-tuned-alpha-7b     1     2.96875\n",
            "\n",
            "########## Second turn ##########\n",
            "                                     score\n",
            "model                       turn          \n",
            "gpt-4                       2     9.025000\n",
            "claude-instant-v1           2     8.012658\n",
            "gpt-3.5-turbo               2     7.812500\n",
            "claude-v1                   2     7.650000\n",
            "wizardlm-30b                2     6.887500\n",
            "vicuna-33b-v1.3             2     6.787500\n",
            "Llama-2-70b-chat            2     6.725000\n",
            "Llama-2-13b-chat            2     6.237500\n",
            "guanaco-33b                 2     6.175000\n",
            "Llama-2-7b-chat             2     6.125000\n",
            "mpt-30b-chat                2     6.112500\n",
            "palm-2-chat-bison-001       2     6.087500\n",
            "guanaco-65b                 2     6.037500\n",
            "vicuna-13b-v1.3             2     5.962500\n",
            "tulu-30b                    2     5.850000\n",
            "oasst-sft-7-llama-30b       2     5.712500\n",
            "wizardlm-13b                2     5.587500\n",
            "vicuna-7b-v1.5              2     5.560976\n",
            "vicuna-7b-v1.3              2     5.300000\n",
            "baize-v2-13b                2     5.181250\n",
            "mpt-7b-chat                 2     5.063291\n",
            "gpt4all-13b-snoozy          2     4.822785\n",
            "mpt-30b-instruct            2     4.762500\n",
            "nous-hermes-13b             2     4.664557\n",
            "koala-13b                   2     4.625000\n",
            "falcon-40b-instruct         2     4.525000\n",
            "alpaca-13b                  2     4.087500\n",
            "chatglm-6b                  2     4.000000\n",
            "h2ogpt-oasst-open-llama-13b 2     3.737500\n",
            "oasst-sft-4-pythia-12b      2     3.662500\n",
            "rwkv-4-raven-14b            2     3.225000\n",
            "dolly-v2-12b                2     2.750000\n",
            "fastchat-t5-3b              2     2.687500\n",
            "stablelm-tuned-alpha-7b     2     2.537500\n",
            "llama-13b                   2     1.950000\n",
            "\n",
            "########## Average ##########\n",
            "                                score\n",
            "model                                \n",
            "gpt-4                        8.990625\n",
            "gpt-3.5-turbo                7.943750\n",
            "claude-instant-v1            7.905660\n",
            "claude-v1                    7.900000\n",
            "vicuna-33b-v1.3              7.121875\n",
            "wizardlm-30b                 7.009375\n",
            "Llama-2-70b-chat             6.856250\n",
            "Llama-2-13b-chat             6.650000\n",
            "guanaco-33b                  6.528125\n",
            "tulu-30b                     6.434375\n",
            "oasst-sft-7-llama-30b        6.409375\n",
            "guanaco-65b                  6.409375\n",
            "palm-2-chat-bison-001        6.400000\n",
            "mpt-30b-chat                 6.393750\n",
            "vicuna-13b-v1.3              6.387500\n",
            "wizardlm-13b                 6.353125\n",
            "Llama-2-7b-chat              6.268750\n",
            "vicuna-7b-v1.5               6.064815\n",
            "vicuna-7b-v1.3               5.996875\n",
            "baize-v2-13b                 5.750000\n",
            "nous-hermes-13b              5.553459\n",
            "mpt-7b-chat                  5.459119\n",
            "gpt4all-13b-snoozy           5.452830\n",
            "koala-13b                    5.350000\n",
            "mpt-30b-instruct             5.218750\n",
            "falcon-40b-instruct          5.168750\n",
            "h2ogpt-oasst-open-llama-13b  4.625000\n",
            "alpaca-13b                   4.531250\n",
            "chatglm-6b                   4.500000\n",
            "oasst-sft-4-pythia-12b       4.318750\n",
            "rwkv-4-raven-14b             3.984375\n",
            "dolly-v2-12b                 3.275000\n",
            "fastchat-t5-3b               3.040625\n",
            "stablelm-tuned-alpha-7b      2.753125\n",
            "llama-13b                    2.606250\n"
          ]
        }
      ]
    }
  ]
}